= Lab: Deploying and Managing OpenShift Container Storage with Rook-Ceph Operator

== Lab Overview

This hands-on workshop is for both system administrators and application developers interested in learning how to deploy and manage OpenShift Container Storage (OCS). In this lab you will be using OpenShift Container Platform (OCP) 4.x and Rook to deploy Ceph as a persistent storage solution for OCP workloads.

=== In this lab you will learn how to

* Configure and deploy containerized Ceph using Rookâ€™s cluster CustomResourceDefinitions (CRD)
* Validate deployment of Ceph Mimic containerized using OpenShift CLI
* Deploy the Rook toolbox to run cn ceph and rados commands
* Create a Persistent Volume (PV) on the Ceph cluster using a Rook OCP StorageClass for deployment of Rails application using a PostgreSQL database.
* Upgrade Ceph version from Mimic to Nautilus using the Rook operator
* Add more storage to the Ceph cluster

.Rook and Kubernetes Architecture 
image::rook_diagram_3.png[]

.Ceph deployed on OpenShift using Rook
image::rook_diagram_4.png[]

[[labexercises]]
:numbered:
== Deploy Ceph using Rook.io

=== Scale OCP cluster and add 3 new nodes

In this section, you will validate the OCP environment has 3 master and 3 worker nodes before increasing the cluster size by 3 worker nodes. The `NAME` of your OCP nodes will be different than shown below.

[source,bash,role="execute"]
----
oc get nodes
----

----
NAME                                         STATUS   ROLES    AGE    VERSION
ip-10-0-135-64.us-east-2.compute.internal    Ready    worker   117m   v1.13.4+da48e8391
ip-10-0-139-44.us-east-2.compute.internal    Ready    master   124m   v1.13.4+da48e8391
ip-10-0-146-50.us-east-2.compute.internal    Ready    worker   117m   v1.13.4+da48e8391
ip-10-0-147-157.us-east-2.compute.internal   Ready    master   124m   v1.13.4+da48e8391
ip-10-0-160-232.us-east-2.compute.internal   Ready    worker   118m   v1.13.4+da48e8391
ip-10-0-169-223.us-east-2.compute.internal   Ready    master   124m   v1.13.4+da48e8391
----

Now you are going to add 3 more OCP compute nodes to cluster using *machinesets*.

[source,bash,role="execute"]
----
oc get machinesets -n openshift-machine-api
----

This will show you the existing *machinesets* used to create the 3 worker nodes in the cluster already. There is a *machineset* for each AWS AZ (us-east-2a, us-east-2b, us-east-2a). Your *machinesets* `NAME` will be different than below. 

----
NAME                                   DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-a26e-rx8bk-worker-us-east-2a   1         1         1       1           127m
cluster-a26e-rx8bk-worker-us-east-2b   1         1         1       1           127m
cluster-a26e-rx8bk-worker-us-east-2c   1         1         1       1           127m
----

To create the 3 new worker nodes with available storage you update 3 *machineset* definition files:

[source,bash,role="execute"]
----
ll {{ HOME_PATH }}/content/support/cluster-workerocs-*.yaml
----

The machineset files do not have the correct `cluster-api-cluster` label for your lab because every environment is unique. Commands below will find the correct value for your environment. 

[NOTE]
====
*Make sure to execute next step for creating CLUSTERID environment variable*
====

[source,bash,role="execute"]
----
CLUSTERID=$(oc get machineset -n openshift-machine-api -o jsonpath='{.items[0].metadata.labels.machine\.openshift\.io/cluster-api-cluster}')
echo $CLUSTERID
----

Using this correct `cluster-api-cluster` label for your lab environment modify each of the 3 *machinesets* downloaded earlier using the variable `$CLUSTERID`. It is a good idea to backup yaml files when executing `sed`.

[source,bash,role="execute"]
----
mkdir {{ HOME_PATH }}/content/support/backup/
cp {{ HOME_PATH }}/content/support/cluster-workerocs-*.yaml --target-directory={{ HOME_PATH }}/content/support/backup/

sed -i "s/cluster-28cf-t22gs/$CLUSTERID/g" {{ HOME_PATH }}/content/support/cluster-workerocs-*.yaml
----

Check that `cluster-api-cluster` label has been changed to `$CLUSTERID`.

[source,bash,role="execute"]
----
grep cluster-api-cluster {{ HOME_PATH }}/content/support/cluster-workerocs-*
----

The label for `cluster-api-cluster` should now match the results of `echo $CLUSTERID` from above for all occurrences.

Now you are ready to create your 3 new OCP worker nodes using these modified *machinesets*.

[source,bash,role="execute"]
----
oc create -f {{ HOME_PATH }}/content/support/cluster-workerocs-us-east-2a.yaml
oc create -f {{ HOME_PATH }}/content/support/cluster-workerocs-us-east-2b.yaml
oc create -f {{ HOME_PATH }}/content/support/cluster-workerocs-us-east-2c.yaml
----

Check that you have new *machines* created. 

[source,bash,role="execute"]
----
oc get machines -n openshift-machine-api
----

They may be in `pending` for sometime so repeat command above until they are in a `running` STATE. The `NAME` of your machines will be different than shown below. 

----
NAME                                            INSTANCE              STATE     TYPE         REGION      ZONE         AGE
cluster-a26e-rx8bk-master-0                     i-097221131442bbfbb   running   m4.xlarge    us-east-2   us-east-2a   174m
cluster-a26e-rx8bk-master-1                     i-0267d53e93238917b   running   m4.xlarge    us-east-2   us-east-2b   174m
cluster-a26e-rx8bk-master-2                     i-01078f70ca5c9edb4   running   m4.xlarge    us-east-2   us-east-2c   174m
cluster-a26e-rx8bk-worker-us-east-2a-rbfzc      i-0866fa2a597f9b55c   running   m5.2xlarge   us-east-2   us-east-2a   174m
cluster-a26e-rx8bk-worker-us-east-2b-pk9k7      i-0c930c718134b2625   running   m5.2xlarge   us-east-2   us-east-2b   174m
cluster-a26e-rx8bk-worker-us-east-2c-v2jfq      i-0d5361d4219903e7c   running   m5.2xlarge   us-east-2   us-east-2c   173m
cluster-a26e-rx8bk-workerocs-us-east-2a-8pnf4   i-0a497998c19a59ba3   running   m5d.large    us-east-2   us-east-2a   4m1s
cluster-a26e-rx8bk-workerocs-us-east-2b-wwcmd   i-0c25eb473e452645d   running   m5d.large    us-east-2   us-east-2b   95s
cluster-a26e-rx8bk-workerocs-us-east-2c-8456v   i-0e0d311e4590fa7e3   running   m5d.large    us-east-2   us-east-2c   91s
----

You can see that the workerocs *machines* are using a different AWS EC2 instance type `m5d.large`. This is because this EC2 instance type comes with a 75GB NVMe SSD that will be used for our storage cluster. Now you want to see if our new *machines* are added to the OCP cluster.

[source,bash,role="execute"]
----
watch oc get machinesets -n openshift-machine-api
----

This step could take more than 5 minutes. The result of this command needs to look like below before you proceed. All new workerocs *machinesets* should have an integer, in this case `1`, filled out for all rows and under columns `READY` and `AVAILABLE`. The `NAME` of your *machinesets* will be different than shown below. 

----
NAME                                      DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-a26e-rx8bk-worker-us-east-2a      1         1         1       1           179m
cluster-a26e-rx8bk-worker-us-east-2b      1         1         1       1           179m
cluster-a26e-rx8bk-worker-us-east-2c      1         1         1       1           179m
cluster-a26e-rx8bk-workerocs-us-east-2a   1         1         1       1           9m4s
cluster-a26e-rx8bk-workerocs-us-east-2b   1         1         1       1           6m38s
cluster-a26e-rx8bk-workerocs-us-east-2c   1         1         1       1           6m34s
----

Now check to see that you have 3 new OCP worker nodes.

[source,bash,role="execute"]
----
oc get nodes -l node-role.kubernetes.io/worker
----

The `NAME` of your OCP nodes will be different than shown below.

----
NAME                                         STATUS   ROLES    AGE     VERSION
ip-10-0-135-6.us-east-2.compute.internal     Ready    worker   5m58s   v1.12.4+30e6a0f55
ip-10-0-135-64.us-east-2.compute.internal    Ready    worker   175m    v1.12.4+30e6a0f55
ip-10-0-146-50.us-east-2.compute.internal    Ready    worker   175m    v1.12.4+30e6a0f55
ip-10-0-156-83.us-east-2.compute.internal    Ready    worker   3m7s    v1.12.4+30e6a0f55
ip-10-0-160-232.us-east-2.compute.internal   Ready    worker   176m    v1.12.4+30e6a0f55
ip-10-0-164-65.us-east-2.compute.internal    Ready    worker   3m30s   v1.12.4+30e6a0f55
...
----
[NOTE]
====
You may have more nodes than shown if you have completed a different lab such as the infrastructure node lab.
====

=== Download Rook deployment files and install Ceph

In this section you will be using the new worker OCP nodes created in last
section along with Rook image and configuration files. You will download
files *common.yaml*, *operator-openshift.yaml*, *cluster.yaml* and
*toolbox.yaml* to create Rook and Ceph resources as shown in Figure 1 and
Figure 2 above.

First, validate that the 3 new OCP worker nodes are labeled with
role=storage-node. This label was configured in each of the *machinesets* you
used in last section so there is no need to manually add this label used for
selecting OCP nodes for Rook deployment.

[source,bash,role="execute"]
----
oc get nodes --show-labels | grep storage-node
----

The first step to deploy Rook is to create the common resources. The
configuration for these resources will be the same for most deployments. The
*common.yaml* sets these resources up.

[source,bash,role="execute"]
----
oc create -f {{ HOME_PATH }}/content/support/common.yaml
----

After the common resources are created, the next step is to create the Operator deployment using *operator-openshift.yaml*.  

[source,bash,role="execute"]
----
oc create -f {{ HOME_PATH }}/content/support/operator-openshift.yaml
watch oc get pods -n rook-ceph
----

Wait for all *rook-ceph-agent*, *rook-discover* and *rook-ceph-operator* pods to be in a `Running` STATUS.

```
NAME                                  READY   STATUS    RESTARTS   AGE
rook-ceph-agent-2fsnb                 1/1     Running   0          33s
rook-ceph-agent-66php                 1/1     Running   0          33s
rook-ceph-agent-7nx95                 1/1     Running   0          33s
rook-ceph-agent-fpcgr                 1/1     Running   0          33s
rook-ceph-agent-pfznq                 1/1     Running   0          33s
rook-ceph-agent-pp4dl                 1/1     Running   0          33s
rook-ceph-agent-rgc27                 1/1     Running   0          33s
rook-ceph-agent-tvc77                 1/1     Running   0          33s
rook-ceph-agent-wtvdm                 1/1     Running   0          33s
rook-ceph-operator-7fd87d4bb9-vtvmj   1/1     Running   0          55s
rook-discover-2kskz                   1/1     Running   0          33s
rook-discover-7t756                   1/1     Running   0          33s
rook-discover-dbfk7                   1/1     Running   0          33s
rook-discover-hzvvn                   1/1     Running   0          33s
rook-discover-jtxln                   1/1     Running   0          33s
rook-discover-mmdml                   1/1     Running   0          33s
rook-discover-qzdhf                   1/1     Running   0          33s
rook-discover-wq4lr                   1/1     Running   0          33s
rook-discover-xb8qt                   1/1     Running   0          33s
```

The log for the *rook-ceph-operator* pod should show that the operator is
looking for a cluster. Look for `the server could not find the requested
resource (get clusters.ceph.rook.io)` in the *rook-ceph-operator* log file.
This means the operator is looking for a Ceph cluster that does not exist
yet.

[source,bash,role="execute"]
----
OPERATOR=$(oc get pod -l app=rook-ceph-operator -n rook-ceph -o jsonpath='{.items[0].metadata.name}')
echo $OPERATOR
oc logs $OPERATOR -n rook-ceph | grep "get clusters.ceph.rook.io"
----

Now that your operator resources are running, the next step is to create your
Ceph storage cluster. This *cluster.yaml* file contains settings for a
production Ceph storage cluster. The minimum deployment requires at least 3
OCP nodes. In this lab these will be the OCP nodes created earlier using the
AWS EC2 `m5d.large` instance type each with an available 75GB NVMe SSD.

[source,bash,role="execute"]
----
cat {{ HOME_PATH }}/content/support/cluster.yaml
----

Take a look at the *cluster.yaml* file. It specifies the version of Ceph and
the label used for the rook resources. This label, `role=storage-node` was
validated as being on our new OCP nodes at the beginning of this section.
Also `useAllNodes=true` and `useAllDevices=true` means that if a OCP node has
label `role=storage-node` then all available storage devices on this node
will be used for the Ceph cluster.

[source,yaml]
----
...
    image: ceph/ceph:v13.2.5-20190410
...

  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: role
              operator: In
              values:
              - storage-node
...

  storage: # cluster level storage configuration and selection
    useAllNodes: true
    useAllDevices: true
    deviceFilter:
    location:
    config:	
...	

----

Now create the Ceph resources.

[source,bash,role="execute"]
----
oc create -f {{ HOME_PATH }}/content/support/cluster.yaml
----

It may take more than 5 minutes to create all of the new *MONs*, *MGR* and *OSD* pods.

[source,bash,role="execute"]
----
watch "oc get pods -n rook-ceph | egrep -v -e rook-discover -e rook-ceph-agent"
----

The `NAME` of your pods will be different than shown below. 

----
NAME                                        READY    STATUS     RESTARTS    AGE
rook-ceph-mgr-a-86b5b58769-xngqm             1/1     Running     0          110s
rook-ceph-mon-a-f95bc46-2jffm                1/1     Running     0          3m13s
rook-ceph-mon-b-54588c7d68-prm8f             1/1     Running     0          2m45s
rook-ceph-mon-c-5567868987-t72zz             1/1     Running     0          2m24s
rook-ceph-operator-9bb6f7745-r7rft           1/1     Running     0          53m
rook-ceph-osd-0-88d4c654-lsz2m               1/1     Running     0          66s
rook-ceph-osd-1-55b49d48df-lvnlv             1/1     Running     0          66s
rook-ceph-osd-2-745b7669d7-gkhl5             1/1     Running     0          66s
rook-ceph-osd-prepare-ip-10-0-135-6-p8rsz    0/2     Completed   0          91s
rook-ceph-osd-prepare-ip-10-0-156-83-tjft2   0/2     Completed   0          91s
rook-ceph-osd-prepare-ip-10-0-164-65-9wq67   0/2     Completed   0          90s
----

Once all pods are in a Running state it is time to verify that Ceph is operating correctly. Download *toolbox.yaml* to run Ceph commands.

[source,bash,role="execute"]
----
oc create -f {{ HOME_PATH }}/content/support/toolbox.yaml
----

Now you can login to *rook-ceph-tools* pod to run Ceph commands. This pod is commonly called the *toolbox*.

[source,bash,role="execute"]
----
export toolbox=$(oc -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}')
oc -n rook-ceph rsh $toolbox
----

Once logged into the *toolbox* (you see a prompt `sh-4.2#`) use commands below to investigate the Ceph status and configuration. 

[source,bash,role="execute"]
----
ceph status
ceph osd status
ceph osd tree
ceph df
rados df
----

Make sure to `exit` the *toolbox* before continuing. 

=== Create Rook storageclass for creating Ceph RBD volumes

In this section you will download *storageclass.yaml* and then create the OCP *storageclass* `rook-ceph-block` that can be used by applications to dynamically claim persistent volumes (*PVCs*). The Ceph pool `replicapool` is created when the OCP *storageclass* is created.

[source,bash,role="execute"]
----
cat {{ HOME_PATH }}/content/support/storageclass.yaml
----

Notice the `provisioner: ceph.rook.io/block` and that `replicated: size=2` which is a good practice when there are only 3 OSDs. This is because if one *OSD* is down OCP volumes can continue to be created and used. 

[source,bash,role="execute"]
----
oc create -f {{ HOME_PATH }}/content/support/storageclass.yaml
----

Login to *toolbox* pod to run Ceph commands. Compare results for `ceph df`
and `rados df` executed in prior section before the new *storageclass* was
created. You will see there is now a Ceph pool called `replicapool`. Also,
the command `ceph osd pool ls detail` gives you information on how this pool
is configured.

[source,bash,role="execute"]
----
oc -n rook-ceph rsh $toolbox
----

[source,bash,role="execute"]
----
ceph df
rados df
rados -p replicapool ls
ceph osd pool ls detail
----

Make sure to `exit` the *toolbox* before continuing.

== Create new OCP deployment using Ceph RBD volume

In this section the `rook-ceph-block` *storageclass* will be used by an OCP
application + database deployment to create persistent storage. The
persistent storage will be a Ceph RBD (RADOS Block Device) volume (object) in
the pool=`replicapool`.

Because the Rails + PostgreSQL deployment uses the `default` *storageclass*
you need to modify the current default, gp2, and then make `rook-ceph-block`
the default *storageclass*.

[source,bash,role="execute"]
----
oc get sc
----

You will see the following *storageclasses*:

----
NAME              PROVISIONER             AGE
gp2 (default)     kubernetes.io/aws-ebs   2d
rook-ceph-block   ceph.rook.io/block      8m27s
----

Now you want to change which *storageclass* is default. 

We will be removing the following annotation from gp2 and add it to rook-ceph-block

[source,yaml]
----
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
----
This should remove the annotation from gp2
[source,bash,role="execute"]
----
oc annotate sc gp2 storageclass.kubernetes.io/is-default-class="false" --overwrite
----


The following will annotate rook-ceph-block so it will become the `default` *storageclass*.

[source,bash,role="execute"]
----
oc annotate sc rook-ceph-block storageclass.kubernetes.io/is-default-class="true"
----

After editing *storageclass* `rook-ceph-block` the result should be similar
to below and `rook-ceph-block` should now be the `default` *storageclass*.

[source,bash,role="execute"]
----
oc get sc rook-ceph-block -o yaml
----

Your *storageclass* will now look like:

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
  creationTimestamp: "2019-04-26T22:24:29Z"
  name: rook-ceph-block
...
----

Validate that `rook-ceph-block` is now the default *storageclass* before starting the OCP application deployment.

[source,bash,role="execute"]
----
oc get sc
----

You will see the default set like this:

----
NAME                        PROVISIONER             AGE
gp2                         kubernetes.io/aws-ebs   2d1h
rook-ceph-block (default)   ceph.rook.io/block      10m32s
----

Now you are ready to start the Rails + PostgreSQL deployment and monitor the deployment. 

[source,bash,role="execute"]
----
oc new-project my-database-app
oc new-app rails-pgsql-persistent -p VOLUME_CAPACITY=5Gi
----

After the deployment is started you can monitor with these commands.

[source,bash,role="execute"]
----
oc status
oc get pvc -n my-database-app
oc get pods -n my-database-app
----

This step could take 5 or more minutes. Wait until there are 2 pods in `Running` STATUS and 4 pods in `Completed` STATUS as shown below. 

----
NAME                                READY   STATUS      RESTARTS   AGE
postgresql-1-deploy                 0/1     Completed   0          5m48s
postgresql-1-lf7qt                  1/1     Running     0          5m40s
rails-pgsql-persistent-1-build      0/1     Completed   0          5m49s
rails-pgsql-persistent-1-deploy     0/1     Completed   0          3m36s
rails-pgsql-persistent-1-hook-pre   0/1     Completed   0          3m28s
rails-pgsql-persistent-1-pjh6q      1/1     Running     0          3m14s
----

Once the deployment is complete you can now test the application and the
persistent storage on Ceph.

[source,bash,role="execute"]
----
oc get route -n my-database-app
----

And you will see:

----
NAME                     HOST/PORT                                                                         PATH   SERVICES                 PORT    TERMINATION   WILDCARD
rails-pgsql-persistent   rails-pgsql-persistent-my-database-app.{{ ROUTE_SUBDOMAIN }}          rails-pgsql-persistent
----

Copy the route path above to a browser window to create articles. You will
need to append `/articles` to the end.

*Select + Click this link:* http://rails-pgsql-persistent-my-database-app.{{ ROUTE_SUBDOMAIN }}/articles

Select the "New Article" link. Enter the `username` and `password` below to
create articles and comments. The articles and comments are saved in a
PostgreSQL database which stores its table spaces on the Ceph RBD volume
provisioned using the `rook-ceph-block` *storagclass* during the application
deployment.

----
username: openshift
password: secret
----

Lets now take another look at the Ceph `replicapool` created by the
`rook-ceph-block` *storageclass*. Log into the *toolbox* pod again.

[source,bash,role="execute"]
----
oc -n rook-ceph rsh $toolbox
----

Run the same Ceph commands as before the application deployment and compare to results in prior section. Notice the number of objects in `replicapool` now.

[source,bash,role="execute"]
----
ceph df
rados df
rados -p replicapool ls | grep pvc
----

Make sure to `exit` the *toolbox*. Validate the OCP *PersistentVolume* (PV) name is the same name as the volume name in the Ceph `replicapool`.

[source,bash,role="execute"]
----
oc get pvc -n my-database-app
----

== Using Rook to Upgrade Ceph

In this section you will upgrade the Ceph version from *Mimic* to *Nautilus*
using the Rook operator. First verify the current version by logging into the
*toolbox* pod.

[source,bash,role="execute"]
----
oc -n rook-ceph rsh $toolbox
ceph version
----

Make sure to `exit` the *toolbox* before continuing.

The result should be:

----
ceph version 13.2.5 (cbff874f9007f1869bfd3821b7e33b2a6ffd4988) mimic (stable)
----

The next thing you need to do is update the cluster CRD with a current *Nautilus* image name and version. 

[source,bash,role="execute"]
----
oc edit cephcluster rook-ceph -n rook-ceph
----

Modify the Ceph version in the cluster CRD. Using `oc edit` is the same as using editing tool `vi`. 

From:

[source,yaml]
----
spec:
  cephVersion:
    image: ceph/ceph:v13.2.5-20190410
----

To the version for Nautilus. Make sure to save your changes before exiting `:wq!`.

To:

[source,yaml]
----
spec:
  cephVersion:
    image: ceph/ceph:v14.2.0-20190410
----

This could step take 5 minutes or more. Once the change to the Ceph version
is edited as shown above, the *MONs*, *MGR*, and *OSD* pods will be
restarted. You can tell that they have been restarted when their `AGE` is
seconds or just a few minutes.

[source,bash,role="execute"]
----
watch "oc get pods -n rook-ceph | egrep -e rook-ceph-mgr -e rook-ceph-mon -e rook-ceph-operator -e rook-ceph-osd"
----

The `NAME` of your pods will be different than shown below. 

----
NAME                                        READY    STATUS     RESTARTS    AGE
rook-ceph-mgr-a-777d64fb8f-q7tcz             1/1     Running     0          2m55s
rook-ceph-mon-a-5c7fcdfcc4-zwzb7             1/1     Running     0          3m18s
rook-ceph-mon-b-5597dbd64d-cdvjf             1/1     Running     0          4m33s
rook-ceph-mon-c-779cbf9bc-2rfl5              1/1     Running     0          3m58s
rook-ceph-operator-5f7967846f-zqqjl          1/1     Running     0          27h
rook-ceph-osd-0-855bc669cd-45sk7             1/1     Running     0          2m8s
rook-ceph-osd-1-7cc9cd8c8c-j9ffl             1/1     Running     0          115s
rook-ceph-osd-2-5977cd8bff-9x85n             1/1     Running     0          98s
----


Now let's check the version of Ceph to see if it is upgraded. First you need
to login to the *toolbox* pod again.

[source,bash,role="execute"]
----
oc -n rook-ceph rsh $toolbox
----

Running the `ceph versions` command shows each of the Ceph daemons (*MONs*,
*MGR*, and *OSD*) have been upgraded to *Nautilus*. Run other Ceph commands
to satisfy yourself (e.g., ceph status) the system is healthy after the Ceph
upgrade from *Mimic* to *Nautilus*. You might even want to go back to the
Rails + PostgreSQL application and save a few more articles to make sure Ceph
storage is still working.

[source,bash,role="execute"]
----
ceph versions
----

Your output will look something like:

[source,json]
----
{
    "mon": {
        "ceph version 14.2.0 (3a54b2b6d167d4a2a19e003a705696d4fe619afc) nautilus (stable)": 3
    },
    "mgr": {
        "ceph version 14.2.0 (3a54b2b6d167d4a2a19e003a705696d4fe619afc) nautilus (stable)": 1
    },
    "osd": {
        "ceph version 14.2.0 (3a54b2b6d167d4a2a19e003a705696d4fe619afc) nautilus (stable)": 3
    },
    "mds": {},
    "overall": {
        "ceph version 14.2.0 (3a54b2b6d167d4a2a19e003a705696d4fe619afc) nautilus (stable)": 7
    }
}
----

Make sure to `exit` the *toolbox* before continuing.

You can also execute this command:

[source,bash,role="execute"]
----
oc -n rook-ceph get deployments -o jsonpath='{range .items[*]}{.metadata.name}{" \trook="}{.metadata.labels.rook-version}{" \tceph="}{.metadata.labels.ceph-version}{"\n"}{end}' | sed s/v0.9.0-557.g48380dd/v1.0.0/g
----

To easily look at both the Rook and Ceph versions currently running for the *MONs*, *MGR* and *OSDs*.

----
rook-ceph-mgr-a 	rook=v1.0.0 	ceph=14.2.0
rook-ceph-mon-a 	rook=v1.0.0 	ceph=14.2.0
rook-ceph-mon-b 	rook=v1.0.0 	ceph=14.2.0
rook-ceph-mon-c 	rook=v1.0.0 	ceph=14.2.0
rook-ceph-osd-0 	rook=v1.0.0 	ceph=14.2.0
rook-ceph-osd-1 	rook=v1.0.0 	ceph=14.2.0
rook-ceph-osd-2 	rook=v1.0.0 	ceph=14.2.0
rook-ceph-osd-3 	rook=v1.0.0 	ceph=14.2.0
----

== Adding storage to the Ceph Cluster

In this section you will add more storage to the cluster by increasing the
number of OCP workerocs *machines* and worker nodes from 3 to 4 using one of
the *machinesets* you already used. The new *machine* will also be an EC2
instance `m5d.large` and have an available 75 GB NVMe SSD. The Rook operator
will `observe` when the new OCP node is added to the cluster and will then
create a new *OSD* pod on this new worker node and the 75 GB SSD will be
added to the Ceph cluster with no additional manual effort from the user.

To increase the number of *machines* and the OCP nodes you will again use a
*machineset*. Each of the *machinesets* you used earlier created just one
machine because of `replicas: 1` in the configuration file. Your
`cluster-api-cluster` and `name` is different than example shown below.

[source,bash,role="execute"]
----
cat machineset {{ HOME_PATH }}/content/support/cluster-workerocs-us-east-2a.yaml | more
----


[source,yaml]
----
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: cluster-a26e-rx8bk
    machine.openshift.io/cluster-api-machine-role: workerocs
    machine.openshift.io/cluster-api-machine-type: workerocs
  name: cluster-a26e-rx8bk-workerocs-us-east-2a
  namespace: openshift-machine-api
spec:
  replicas: 1
...
----

Verify your `cluster-api-cluster` again by using the command below.

[source,bash,role="execute"]
----
echo $CLUSTERID
----

You can easily create a new *machine* and OCP worker node in AWS AZ
us-east-2a by just increasing the `replicas` count in one of the machinesets.
Edit your machineset for us-east-2a to increase from `replicas: 1` to
`replicas: 2`. Make sure to save your changes before exiting `:wq!`.

[source,bash,role="execute"]
----
oc edit machineset $CLUSTERID-workerocs-us-east-2a -n openshift-machine-api
----

Verify you now have 4 workerocs *machines*.

[source,bash,role="execute"]
----
oc get machines -n openshift-machine-api
----

One of the *machines* should have just been created in us-east-2a AZ so there
are two in this AZ now. The `NAME` of your *machines* will be different than
shown below.

----
NAME                                            INSTANCE              STATE     TYPE         REGION      ZONE         AGE
...
cluster-a26e-rx8bk-workerocs-us-east-2a-8pnf4   i-0a497998c19a59ba3   running   m5d.large    us-east-2   us-east-2a   2d
cluster-a26e-rx8bk-workerocs-us-east-2a-l4v5l   i-0e22f1078f1228086   running   m5d.large    us-east-2   us-east-2a   33s
cluster-a26e-rx8bk-workerocs-us-east-2b-wwcmd   i-0c25eb473e452645d   running   m5d.large    us-east-2   us-east-2b   47h
cluster-a26e-rx8bk-workerocs-us-east-2c-8456v   i-0e0d311e4590fa7e3   running   m5d.large    us-east-2   us-east-2c   47h
----

Now you need to verify that this new *machine* is added to the OCP cluster.

This step could take more than 5 minutes. You can see now that one of the
*machinesets* has 2 machines, this is because you increased the replica count
in the prior step. The *machineset* for us-east-2a should have an integer, in
this case `2`, filled out for the entire row before you proceed to the next
step.

[source,bash,role="execute"]
----
watch oc get machinesets -n openshift-machine-api
----

The `NAME` of your machinesets will be different than shown below. 

----
NAME                                      DESIRED   CURRENT   READY   AVAILABLE   AGE
...
cluster-a26e-rx8bk-workerocs-us-east-2a   2         2         2       2           2d
cluster-a26e-rx8bk-workerocs-us-east-2b   1         1         1       1           2d
cluster-a26e-rx8bk-workerocs-us-east-2c   1         1         1       1           2d
----

Now verify that you have a new OCP worker node.

[source,bash,role="execute"]
----
oc get nodes -l node-role.kubernetes.io/worker
----

You should now have 7 worker nodes. You will have more if you have completed
the Infrastructure Nodes and Operators lab.

----
NAME                                         STATUS   ROLES    AGE     VERSION
ip-10-0-135-6.us-east-2.compute.internal     Ready    worker   2d      v1.13.4+da48e8391
ip-10-0-135-64.us-east-2.compute.internal    Ready    worker   2d2h    v1.13.4+da48e8391
ip-10-0-137-156.us-east-2.compute.internal   Ready    worker   4m28s   v1.13.4+da48e8391
ip-10-0-146-50.us-east-2.compute.internal    Ready    worker   2d2h    v1.13.4+da48e8391
ip-10-0-156-83.us-east-2.compute.internal    Ready    worker   2d      v1.13.4+da48e8391
ip-10-0-160-232.us-east-2.compute.internal   Ready    worker   2d2h    v1.13.4+da48e8391
ip-10-0-164-65.us-east-2.compute.internal    Ready    worker   2d      v1.13.4+da48e8391
----

Until Openshift Container Platform 4.2 rolls out, we will need to restart (delete) the operator pod to see OSD pod added.
[source,bash,role="execute"]
----
oc delete pod -l app=rook-ceph-operator -n rook-ceph
----

This step could take 5 minutes or more for the forth *OSD* pod to be in a `Running` STATUS.

[source,bash,role="execute"]
----
watch oc get pods -n rook-ceph
----

Eventually you will see a new *OSD* pod, `rook-ceph-osd-3`, that has just been created.  

----
NAME                                          READY   STATUS      RESTARTS   AGE
...
rook-ceph-osd-0-855bc669cd-45sk7              1/1     Running     0          55m
rook-ceph-osd-1-7cc9cd8c8c-j9ffl              1/1     Running     0          55m
rook-ceph-osd-2-5977cd8bff-9x85n              1/1     Running     0          55m
rook-ceph-osd-3-56b6c4f459-q7mhz              1/1     Running     0          114s
...

----

Now you can validate that Ceph is healthy and has the additional storage. You again login to the *toolbox* pod.

[source,bash,role="execute"]
----
oc -n rook-ceph rsh $toolbox
----

And run Ceph commands to see the new OSDs.

[source,bash,role="execute"]
----
ceph osd status
----

You will see new OSDs as below:

----
+----+--------------------------------------------+-------+-------+--------+---------+--------+---------+-----------+
| id |                    host                    |  used | avail | wr ops | wr data | rd ops | rd data |   state   |
+----+--------------------------------------------+-------+-------+--------+---------+--------+---------+-----------+
| 0  |  ip-10-0-135-6.us-east-2.compute.internal  | 1051M | 68.8G |    0   |     0   |    0   |     0   | exists,up |
| 1  | ip-10-0-156-83.us-east-2.compute.internal  | 1060M | 68.8G |    0   |     0   |    0   |     0   | exists,up |
| 2  | ip-10-0-164-65.us-east-2.compute.internal  | 1062M | 68.8G |    0   |     0   |    0   |     0   | exists,up |
| 3  | ip-10-0-137-156.us-east-2.compute.internal | 1061M | 67.9G |    0   |     0   |    0   |     0   | exists,up |
+----+--------------------------------------------+-------+-------+--------+---------+--------+---------+-----------+
----

And you can see that Ceph is healthy and happy! There are now 4 *OSDs* `up`
and `in`. You might even want to go back to the the Rails + PostgreSQL
application and save a few more articles to make sure applications using Ceph
storage are still working.

[source,bash,role="execute"]
----
ceph status
----

Verify status is `HEALTH_OK`:

----
  cluster:
    id:     90306026-6e42-4877-9d4e-26eca2ecf6ef
    health: HEALTH_OK
 
  services:
    mon: 3 daemons, quorum b,a,c (age 59m)
    mgr: a(active, since 5m)
    osd: 4 osds: 4 up, 4 in
 
  data:
    pools:   1 pools, 100 pgs
    objects: 36 objects, 73 MiB
    usage:   4.1 GiB used, 274 GiB / 279 GiB avail
    pgs:     100 active+clean
----

Make sure to `exit` the *toolbox*. 
