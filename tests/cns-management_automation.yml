---

  - name: Automation of module 6 (CNS Management)
    hosts: localhost

    tasks:

      - include_vars: /opt/lab/environment.yml
        tags: always

      - set_fact:
          cns_namespace: "{{ CNS_NAMESPACE }}"
          cns_storageclass: "{{ CNS_STORAGECLASS }}"
          new_cns_nodes_internal_fqdn:
            - "{{NODE4_INTERNAL_FQDN}}"
            - "{{NODE5_INTERNAL_FQDN}}"
            - "{{NODE6_INTERNAL_FQDN}}"
          heketi_resturl: "http://heketi-{{CNS_NAMESPACE}}.{{OCP_ROUTING_SUFFIX}}"
          heketi_admin_pw: "{{ HEKETI_ADMIN_PW }}"
          cns_storageclass2: "{{CNS_STORAGECLASS2}}"
          ocp_routing_suffix: "{{OCP_ROUTING_SUFFIX}}"
          node_brick_device: "{{ NODE_BRICK_DEVICE }}"
          node_brick_device2: "{{ NODE_BRICK_DEVICE2 }}"
          node4_internal_fqdn: " {{ NODE4_INTERNAL_FQDN }}"
        tags: always


      - block:

        - name: login as fancyuser1
          shell: oc login -u 'fancyuser1' -p 'openshift'

        - name: create new project called 'my-database-app'
          shell: oc new-project my-database-app

        - name: export the rails-postgres-persistent template
          shell: oc export template/rails-pgsql-persistent -n openshift -o yaml > rails-app-template.yml
          args:
            creates: rails-app-template.yml

        - name: render the rails-postgres-persistent template with custom volume capacity
          shell: oc process -f rails-app-template.yml -o yaml -p VOLUME_CAPACITY=15Gi > my-rails-app.yml
          args:
            creates: my-rails-app.yml

        - name: deploy app
          shell: oc create -f my-rails-app.yml

        - name: wait for postgres app to be ready
          shell: oc get rc -o jsonpath='{$.items[?(@.spec.selector.name=="rails-pgsql-persistent")].status.readyReplicas}'
          register: pgsql_app_rc_check
          until: pgsql_app_rc_check.stdout == "1"
          retries: 60
          delay: 10

        - name: check pvc
          shell: oc get pvc/postgresql

        - name: check route
          shell: oc get route/rails-pgsql-persistent

        - name: re-login as system:admin
          shell: oc login -u system:admin

        - name: change to project of fancyuser1
          shell:  oc project my-database-app

        - name: get pvc
          shell: oc get pvc/postgresql -o jsonpath='{$.spec.volumeName}'
          register: pgsql_pv
          failed_when: pgsql_pv.stdout == ""

        - set_fact:
            rwo_pv: "{{ pgsql_pv.stdout }}"

        - name: get pv
          shell: oc get pv/{{ rwo_pv }} -o jsonpath='{$.spec.glusterfs.path}'
          register: pgsql_vol
          failed_when: pgsql_vol.stdout == ""

        - set_fact:
            rwo_gvol: "{{ pgsql_vol.stdout }}"

        - name: get first glusterfs pods hostIP
          shell: oc get pods -n {{ cns_namespace }} -o jsonpath='{$.items[0].metadata.name}'
          register: first_glusterfs_pod
          failed_when: first_glusterfs_pod.stdout == ""

        - set_fact:
            gluster_pod: "{{ first_glusterfs_pod.stdout }}"

        - name: search for gluster vol based on pv
          shell: oc rsh -n {{ cns_namespace }} {{ gluster_pod }} gluster vol list
          register: gluster_vol_list
          failed_when: gluster_vol_list.rc != 0 or rwo_gvol not in gluster_vol_list.stdout

        tags:
          - rwo-example

      - block:

        - name: log back in as fancyuser1
          shell: oc login -u 'fancyuser1' -p 'openshift'

        - name: create new project called 'my-shared-storage'
          shell: oc new-project my-shared-storage

        - name: deploy file-uploader app
          shell: oc new-app openshift/php:7.0~https://github.com/christianh814/openshift-php-upload-demo --name=file-uploader

        - name: wait for php app to be ready
          shell: oc get rc -o jsonpath='{$.items[?(@.spec.selector.deploymentconfig=="file-uploader")].status.readyReplicas}'
          register: php_app_rc_check
          until: php_app_rc_check.stdout == "1"
          retries: 30
          delay: 10

        - name: expose php app service
          shell: oc expose svc/file-uploader

        - name: scale php app
          shell: oc scale dc/file-uploader --replicas=3

        - name: wait for php app to be scaled to 3 pods
          shell: oc get dc -o jsonpath='{$.items[?(@.metadata.name=="file-uploader")].status.availableReplicas}'
          register: php_app_scale_check
          until: php_app_scale_check.stdout == "3"
          retries: 12
          delay: 10

        - name: create rwx pvc file from template
          template:
            src: ../templates/cns-rwx-pvc.yml.j2
            dest: /tmp/cns-rwx-pvc.yml

        - name: create rwx pvc
          shell: |
            oc create -f /tmp/cns-rwx-pvc.yml

        - name: wait for pvc to be bound
          shell: oc get pvc/my-shared-storage -o jsonpath='{$.status.phase}'
          register: rwx_pvc
          until: rwx_pvc.stdout == "Bound"
          retries: 6
          delay: 5

        - name: add pvc to app deploymentconfig
          shell: oc volume dc/file-uploader --add --name=shared-storage --type=persistentVolumeClaim --claim-name=my-shared-storage --mount-path=/opt/app-root/src/uploaded

        - name: wait for php app replication controller to be updated
          shell: oc get rc -o jsonpath='{$.items[?(@.spec.selector.deploymentconfig=="file-uploader")].spec.template.spec.volumes[?(@.persistentVolumeClaim.claimName=="my-shared-storage")].name}'
          register: php_app_update_rc_check
          until: php_app_update_rc_check.stdout == "shared-storage"
          retries: 12
          delay: 10

        - name: wait for php app to be updated on all 3 pods
          shell: oc get dc -o jsonpath='{$.items[?(@.spec.selector.deploymentconfig=="file-uploader")].status.updatedReplicas}'
          register: php_app_update_check
          until: php_app_update_check.stdout == "3"
          retries: 12
          delay: 10

        tags:
          - rwx-example

      - block:

        - name: log in as cluster admin
          shell: oc login -u system:admin

        - name: change to cns namespace
          shell: oc project {{cns_namespace}}

        - name: ensure additional hosts are part of cns group
          lineinfile:
            line: "{{ item }}"
            path: /etc/ansible/hosts
            insertafter: "^\\[cns\\]"
            regexp: "^#{{ item }}"
          with_items: "{{ new_cns_nodes_internal_fqdn }}"

        - name: insert iptables rules required for GlusterFS
          blockinfile:
            dest: /etc/sysconfig/iptables
            block: |
              -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24007 -j ACCEPT
              -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24008 -j ACCEPT
              -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 2222 -j ACCEPT
              -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m multiport --dports 49152:49664 -j ACCEPT
            insertbefore: "^COMMIT"
          delegate_to: "{{ item }}"
          with_items: "{{ new_cns_nodes_internal_fqdn }}"

        - name: reload iptables
          systemd:
            name: iptables
            state: reloaded
          delegate_to: "{{ item }}"
          with_items: "{{ new_cns_nodes_internal_fqdn }}"

        - name: label storage nodes
          shell: oc label node/{{ item }} storagenode=glusterfs
          with_items: "{{ new_cns_nodes_internal_fqdn }}"

        - name: check daemonset
          shell: oc get daemonset -o jsonpath='{$.items[?(@.metadata.name=="glusterfs")].status.desiredNumberScheduled}' -n {{cns_namespace}}
          register: check_daemonset_desired
          failed_when: check_daemonset_desired.stdout == ""
          changed_when: False

        - name: wait for daemonset to have desired nodes ready
          shell: oc get daemonset -o jsonpath='{$.items[?(@.metadata.name=="glusterfs")].status.numberReady}' -n {{cns_namespace}}
          register: check_daemonset_ready
          until: check_daemonset_ready.stdout|int == check_daemonset_desired.stdout|int
          failed_when: check_daemonset_ready.stdout == ""
          retries: 12
          delay: 10

        - name: get first cluster-id from heketi
          shell: heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} cluster list --json
          register: heketi_cluster_list_first

        - set_fact:
            heketi_cluster_first: "{{heketi_cluster_list_first.stdout|from_json}}"

        - set_fact:
            cns_clusterid_1: "{{heketi_cluster_first.clusters|first}}"

        - name: load new topology
          shell: heketi-cli topology --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} load --json=topology-extended.json
          args:
            chdir: /home/cloud-user
          register: heketi_topology_load
          failed_when: '"Unable" in heketi_topology_load.stdout'

        - name: get second cluster-id from heketi
          shell: heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} cluster list --json
          register: heketi_cluster_list_second

        - set_fact:
            heketi_cluster_second: "{{heketi_cluster_list_second.stdout|from_json}}"

        - set_fact:
            cns_clusterid_2: "{{heketi_cluster_second.clusters|difference([cns_clusterid_1])|first}}"

        - name: create storage class file from template
          template:
            src: ../templates/cns-storageclass-silver.yml.j2
            dest: /tmp/cns-storageclass-silver.yml

        - name: create storage class
          shell: oc create -f /tmp/cns-storageclass-silver.yml

        - name: query nodes of second cns cluster
          shell: heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} cluster info {{ cns_clusterid_2 }} --json
          register: heketi_cluster_node_list

        - set_fact:
            cns_cluster_info: "{{ heketi_cluster_node_list.stdout|from_json }}"

        - name: add additional devices to second CNS cluster
          shell: heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} device add --name={{ node_brick_device2 }} --node={{ item }}
          with_items: "{{ cns_cluster_info.nodes }}"

        tags:
          - second-cns

      - block:

        - name: get heketi ID of first node of second CNS cluster
          shell: "heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} topology info | grep -B4 {{ node4_internal_fqdn }} | awk 'match($0, /Node Id: ([a-z0-9].*)/, result) { print result[1] }'"
          register: heketi_node_4_query

        - set_fact:
            heketi_node_4_id: "{{ heketi_node_4_query.stdout }}"

        - name: get heketi ID of first device of this node
          shell: "heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} node info {{ heketi_node_4_id }} | grep {{node_brick_device}} | awk 'match($0, /Id:([a-z0-9].*) Name:/, result) { print result[1] }'"
          register: heketi_node_4_device_query

        - set_fact:
            heketi_node_4_device_id: "{{ heketi_node_4_device_query.stdout }}"

        - name: disable the device
          shell: heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} device disable {{ heketi_node_4_device_id }}

        - name: remove the device
          shell: heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} device remove {{ heketi_node_4_device_id }}

        - name: delete the device
          shell: heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} device delete {{ heketi_node_4_device_id }}

        - name: log back in as cluster admin into default namespace
          shell:

        - name: create registry rwx pvc file from template
          template:
            src: ../templates/registry-rwx-pvc.yml.j2
            dest: /tmp/registry-rwx-pvc.yml

        tags:
          - remove-device

      - block:

        - name: create registry pvc
          shell: oc create -f /tmp/registry-rwx-pvc.yml

        - name: wait for pvc to be bound
          shell: oc get pvc/registry-storage -o jsonpath='{$.status.phase}'
          register: registry_pvc
          until: registry_pvc.stdout == "Bound"
          retries: 6
          delay: 5

        - name: update registry deployment
          shell: oc volume deploymentconfigs/docker-registry --add --name=registry-storage -t pvc  --claim-name=registry-storage --overwrite

        - name: wait for registry deployment to be ready
          shell: oc get rc -o jsonpath='{$.items[?(@.spec.selector.deploymentconfig=="docker-registry")].status.readyReplicas}'
          register: registry_rc_check
          until: registry_rc_check.stdout == "1"
          retries: 30
          delay: 10

        - name: scale the registry deployment to 3
          shell: oc scale dc/docker-registry --replicas=3

        - name: wait for registry to be scaled to 3 pods
          shell: oc get dc -o jsonpath='{$.items[?(@.metadata.name=="docker-registry")].status.availableReplicas}'
          register: registry_scale_check
          until: registry_scale_check.stdout == "3"
          retries: 12
          delay: 10

        tags:
          - cns-registry
...
