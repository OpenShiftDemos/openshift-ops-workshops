---
- name: Tests for verifying the OpenShift installation
  hosts: masters
  tags:
    - deployment_verify
  tasks:
    # variable file is autogenerated during cloudformation provisioning
    - include_vars: /opt/lab/environment.yml

    - set_fact:
        api_health_url: "{{ API_HEALTH_URL }}"
        ocp_routing_suffix: "{{ OCP_ROUTING_SUFFIX }}"

    - name: Check that oc client is v3.11.98
      command: oc version
      register: version
      changed_when: false

    - assert:
        that:
          - "'v3.11.98' in version.stdout"
          - "'v1.11.0+d4cacc0' in version.stdout"

    - name: Checking status of all the nodes to be 'Ready'
      command: oc get -o jsonpath='{.status.conditions[?(@.reason=="KubeletReady")].type}' node {{ item }}
      with_items:
        - "{{ groups.nodes }}"
      register: status_of_node
      changed_when: false

    - assert:
        that:
          - "'Ready' in item.stdout"
      with_items:
        - "{{ status_of_node.results }}"

    - name: Validate the public address
      uri:
        url: "{{ api_health_url }}"
        validate_certs: False
        status_code: 200
        method: GET
      changed_when: false

- name: Actions to automate user actions after OpenShift installation
  hosts: masters
  tags:
    - deployment_simulate
  tasks:
    # variable file is autogenerated during cloudformation provisioning
    - include_vars: /opt/lab/environment.yml

    - name: login to OCP using system:admin
      shell: oc login -u system:admin -n default
      register: login
      changed_when: false

    - assert:
        that:
          - "'Logged into \"https://master.internal.aws.testdrive.openshift.com:443\" as \"system:admin\" using existing credentials.' in login.stdout"
          - "'project \"default\".' in login.stdout"

    - name: list all nodes
      shell: oc get nodes
      register: nodes
      changed_when: false

    - set_fact:
        infra_fqdn: "{{ INFRA_INTERNAL_FQDN }}"
        master_fqdn: "{{ MASTER_INTERNAL_FQDN }}"
        node1_fqdn: "{{ NODE1_INTERNAL_FQDN }}"
        node2_fqdn: "{{ NODE2_INTERNAL_FQDN }}"
        node3_fqdn: "{{ NODE3_INTERNAL_FQDN }}"

    - name: ensure nodes are present
      assert:
        that:
          - "infra_fqdn in nodes.stdout"
          - "master_fqdn in nodes.stdout"
          - "node1_fqdn in nodes.stdout"
          - "node2_fqdn in nodes.stdout"
          - "node3_fqdn in nodes.stdout"

    - name: check that there is a prometheus statefulset
      shell: oc get statefulset prometheus-k8s -n openshift-monitoring

    - name: check that prometheus components are running
      shell: oc get pod prometheus-k8s-0 --template='{.status.containerStatuses[?(@.name=="{{ item|quote }}")].ready}' -o jsonpath -n openshift-monitoring
      register: prometheus_status_out
      failed_when: "'true' not in prometheus_status_out.stdout"
      with_items:
        - "prometheus-proxy"
        - "prometheus"

    - name: check that alertmanager components are running
      shell: oc get pod alertmanager-main-0 --template='{.status.containerStatuses[?(@.name=="{{ item|quote }}")].ready}' -o jsonpath -n openshift-monitoring
      register: alertmanager_status_out
      failed_when: "'true' not in alertmanager_status_out.stdout"
      with_items:
        - "alertmanager-proxy"
        - "alertmanager"

    - name: check for prometheus-related routes
      shell: oc get route {{ item }} -n openshift-monitoring
      with_items:
        - "alertmanager-main"
        - "grafana"
        - "prometheus-k8s"

    # there should be 5 nodes, including master, at this point, with exporters
    - name: check for 5 prometheus node exporters
      shell: oc get daemonset node-exporter -o jsonpath --template='{.status.numberReady}' -n openshift-monitoring
      register: prometheus_exporter_out
      failed_when: "'5' not in prometheus_exporter_out.stdout"

    - set_fact:
        cns_namespace: "{{ CNS_NAMESPACE }}"
        ocp_routing_suffix: "{{ OCP_ROUTING_SUFFIX }}"
        heketi_admin_pw: "{{ HEKETI_ADMIN_PW }}"

    - name: list heketi cluster
      shell: heketi-cli cluster list
      register: heketi
      changed_when: false
      environment:
        HEKETI_CLI_SERVER: http://heketi-storage-{{ cns_namespace }}.{{ ocp_routing_suffix }}
        HEKETI_CLI_USER: admin
        HEKETI_CLI_KEY: "{{ heketi_admin_pw }}"

    - name: ensure there is a CNS cluster with file and block enabled
      assert:
        that:
          - "'[file][block]' in heketi.stdout"

    - name: query heketi topology
      shell: heketi-cli topology info
      changed_when: false
      environment:
        HEKETI_CLI_SERVER: http://heketi-storage-{{ cns_namespace }}.{{ ocp_routing_suffix }}
        HEKETI_CLI_USER: admin
        HEKETI_CLI_KEY: "{{ heketi_admin_pw }}"
