---
- name: Cluster scale, logging, metrics, networking
  hosts: localhost
  become: true
  tasks:
    - include_vars: /opt/lab/environment.yml
      tags: always

    - set_fact:
        cns_storageclass: "{{ CNS_STORAGECLASS }}"
        cns_block_storageclass: "{{ CNS_BLOCK_STORAGECLASS }}"
        cns_infra_storageclass: "{{ CNS_INFRA_STORAGECLASS }}"
      tags: always

    - name: Login as system:admin
      command: oc login -u system:admin

    - name: Login to default namespace
      command: oc project default

    - name: uncomment scaleup lines
      replace:
        regexp: '#scaleup_'
        replace: ''
        path: /etc/ansible/hosts
      tags:
        - scaleup

    - name: Check for extra nodes before trying to run scaleup
      command: oc get node node04.internal.aws.testdrive.openshift.com
      register: node_present
      ignore_errors: true
      tags:
        - scaleup

    - name: Run the scaleup playbook
      command: ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/openshift-node/scaleup.yml
      when: node_present | failed
      tags:
        - scaleup

    - name: Check for additional node
      command: oc get node {{ item }}
      with_items:
        - "node04.internal.aws.testdrive.openshift.com"
        - "node05.internal.aws.testdrive.openshift.com"
        - "node06.internal.aws.testdrive.openshift.com"
      tags:
        - scaleup
        - scaleup_test

    - name: Uncomment cnsinfra lines
      replace:
        regexp: '#cnsinfra_'
        replace: ''
        path: /etc/ansible/hosts
      tags:
        - cns_infra
        - cns_infra_deploy

    - name: check for existing cns infra block storage classes
      shell: oc get storageclass {{ cns_block_storageclass }}
      ignore_errors: true
      register: cns_infra_block_check
      tags:
        - cns_infra
        - cns_infra_deploy

    - name: check for existing cns infra file storage classes
      shell: oc get storageclass {{ cns_infra_storageclass }}
      ignore_errors: true
      register: cns_infra_check
      tags:
        - cns_infra
        - cns_infra_deploy

    - name: install the second cns cluster
      shell: ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/openshift-glusterfs/config.yml
      when:
        - cns_infra_check is failed
        - cns_infra_block_check is failed
      tags:
        - cns_infra
        - cns_infra_deploy

    - name: check for cns infra block storage classes
      shell: oc get storageclass {{ cns_block_storageclass }}
      tags:
        - cns_infra
        - cns_infra_test

    - name: check for cns infra file storage classes
      shell: oc get storageclass {{ cns_infra_storageclass }}
      tags:
        - cns_infra
        - cns_infra_test

    - name: issue test rwo pvc against cns infra block storage
      shell: oc process -p PVC_NAME=cns-infra-block-test -p STORAGE_CLASS={{ cns_block_storageclass }} -f templates/cns-rwo-pvc-template.yml -n default | oc create -f -
      tags:
        - cns_infra
        - cns_infra_test

    - name: wait for pvc to be bound
      shell: oc get pvc/cns-infra-block-test -o jsonpath='{$.status.phase}' -n default
      register: check_cns_infra_block_pvc
      until: check_cns_infra_block_pvc.stdout == "Bound"
      retries: 60
      delay: 10
      tags:
        - cns_infra
        - cns_infra_test

    - name: check pvc exists
      shell: oc get pvc/cns-infra-block-test -o json -n default
      register: get_cns_infra_block_pvc
      changed_when: false
      tags:
        - cns_infra
        - cns_infra_test

    - set_fact:
        cns_infra_block_pvc: "{{ get_cns_infra_block_pvc.stdout|from_json }}"
      tags:
        - cns_infra
        - cns_infra_test

    - name: ensure pvc is issued against CNS
      assert:
        that:
          - cns_infra_block_pvc.metadata.annotations['volume.beta.kubernetes.io/storage-provisioner'] == 'gluster.org/glusterblock'
          - cns_infra_block_pvc.spec.storageClassName == cns_block_storageclass
        msg: pvc is not served from StorageClass {{ cns_block_storageclass }}
      tags:
        - cns_infra
        - cns_infra_test

    - name: delete pvc
      shell: oc delete pvc cns-infra-block-test -n default
      tags:
        - cns_infra
        - cns_infra_test

    - name: issue test rwx pvc against cns infra storage
      shell: oc process -p PVC_NAME=cns-infra-test -p STORAGE_CLASS={{ cns_infra_storageclass }} -f templates/cns-rwx-pvc-template.yml -n default | oc create -f -
      tags:
        - cns_infra
        - cns_infra_test

    - name: wait for pvc to be bound
      shell: oc get pvc/cns-infra-test -o jsonpath='{$.status.phase}' -n default
      register: check_cns_infra_pvc
      until: check_cns_infra_pvc.stdout == "Bound"
      retries: 60
      delay: 10
      tags:
        - cns_infra
        - cns_infra_test

    - name: check pvc exists
      shell: oc get pvc/cns-infra-test -o json -n default
      register: get_cns_infra_pvc
      changed_when: false
      tags:
        - cns_infra
        - cns_infra_test

    - set_fact:
        cns_infra_pvc: "{{ get_cns_infra_pvc.stdout|from_json }}"
      tags:
        - cns_infra
        - cns_infra_test

    - name: ensure pvc is issued against CNS
      assert:
        that:
          - cns_infra_pvc.metadata.annotations['volume.beta.kubernetes.io/storage-provisioner'] == 'kubernetes.io/glusterfs'
          - cns_infra_pvc.spec.storageClassName == cns_infra_storageclass
        msg: pvc is not served from StorageClass {{ cns_infra_storageclass }}
      tags:
        - cns_infra
        - cns_infra_test

    - name: delete pvc
      shell: oc delete pvc cns-infra-test -n default
      tags:
        - cns_infra
        - cns_infra_test

    - name: Uncomment metrics lines
      replace:
        regexp: '#metrics_'
        replace: ''
        path: /etc/ansible/hosts
      tags:
        - metrics

    - name: Remove metrics false line
      lineinfile:
        path: /etc/ansible/hosts
        state: absent
        regexp: 'openshift_metrics_install_metrics=false'
      tags:
        - metrics

    - name: Check for metrics before trying to install metrics
      command: oc get service hawkular-cassandra -n openshift-infra
      register: hawkular_cassandra_service
      ignore_errors: true
      tags:
        - metrics

    - name: Remove gluster-file as the system-wide default
      command: "oc patch storageclass {{ cns_storageclass }} -p '{\"metadata\": {\"annotations\": {\"storageclass.kubernetes.io/is-default-class\": \"false\"}}}'"
      tags:
        - metrics
        - logging

    - name: Set gluster-block as the system-wide default
      command: "oc patch storageclass {{ cns_block_storageclass }} -p '{\"metadata\": {\"annotations\": {\"storageclass.kubernetes.io/is-default-class\": \"true\"}}}'"
      tags:
        - metrics
        - logging

    - name: Run the metrics installation playbook
      command: ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/openshift-metrics/config.yml
      when: hawkular_cassandra_service | failed
      tags:
        - metrics

    - name: Determine hawkular-cassandra rc
      shell: oc get rc -n openshift-infra | awk '/hawkular-cassandra*/{ print $1 }'
      register: hawkular_cassandra_rc
      tags:
        - metrics
        - metrics_test

    - name: Wait for Cassandra to be running
      command: oc get rc -n openshift-infra {{ hawkular_cassandra_rc.stdout }} -o jsonpath='{.status.availableReplicas}'
      register: result
      until: '"1" in result.stdout'
      retries: 5
      delay: 60
      tags:
        - metrics
        - metrics_test

    - name: Wait for hawkular to be running
      command: oc get rc -n openshift-infra hawkular-metrics -o jsonpath='{.status.availableReplicas}'
      register: result
      until: '"1" in result.stdout'
      retries: 5
      delay: 60
      tags:
        - metrics
        - metrics_test

    - name: Wait for heapster to be running
      command: oc get rc -n openshift-infra heapster -o jsonpath='{.status.availableReplicas}'
      register: result
      until: '"1" in result.stdout'
      retries: 5
      delay: 60
      tags:
        - metrics
        - metrics_test

    - name: Uncomment logging lines
      replace:
        regexp: '#logging_'
        replace: ''
        path: /etc/ansible/hosts
      tags:
        - logging

    - name: Remove logging false line
      lineinfile:
        path: /etc/ansible/hosts
        state: absent
        regexp: 'openshift_logging_install_logging=false'
      tags:
        - logging

    - name: Check for logging before trying to install logging
      command: oc get service logging-es-cluster -n logging
      register: logging_es_cluster_service
      ignore_errors: true
      tags:
        - logging

    - name: Run the logging installation playbook
      command: ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/openshift-logging/config.yml
      when: logging_es_cluster_service | failed
      tags:
        - logging

    - name: Determine elasticsearch dc
      shell: oc get dc -n logging | awk '/logging-es-*/{ print $1 }'
      register: logging_es_dc
      tags:
        - logging
        - logging_test

    - name: Wait for elasticsearch to be running
      command: oc get dc -n logging {{ logging_es_dc.stdout }} -o jsonpath='{.status.readyReplicas}'
      register: result
      until: '"1" in result.stdout'
      retries: 5
      delay: 60
      tags:
        - logging
        - logging_test

    - name: Wait for curator to be running
      command: oc get dc -n logging logging-curator -o jsonpath='{.status.availableReplicas}'
      register: result
      until: '"1" in result.stdout'
      retries: 5
      delay: 60
      tags:
        - logging
        - logging_test

    - name: Wait for kibana to be running
      command: oc get dc -n logging logging-kibana -o jsonpath='{.status.availableReplicas}'
      register: result
      until: '"1" in result.stdout'
      retries: 5
      delay: 60
      tags:
        - logging
        - logging_test

    # This assumes that this test is being run after the cluster has been scaled
    - name: Wait for fluentd daemonset instances to be running
      command: oc get daemonset logging-fluentd -n logging -o jsonpath='{.status.numberReady}'
      register: result
      until: '"8" in result.stdout'
      retries: 5
      delay: 60
      tags:
        - logging
        - logging_test

    - name: Remove gluster-block as the system-wide default
      command: "oc patch storageclass {{ cns_block_storageclass }} -p '{\"metadata\": {\"annotations\": {\"storageclass.kubernetes.io/is-default-class\": \"false\"}}}'"
      tags:
        - metrics
        - logging

    - name: Set gluster-file as the system-wide default
      command: "oc patch storageclass {{ cns_storageclass }} -p '{\"metadata\": {\"annotations\": {\"storageclass.kubernetes.io/is-default-class\": \"true\"}}}'"
      tags:
        - metrics
        - logging

    # failure of this command (nonzero exit) means it isn't
    - name: Check that multitenant SDN is installed by looking for netnamespaces
      command: oc get netnamespace
      tags:
        - networking
        - networking_test

    - name: Check for netproj-a
      command: oc get project netproj-a
      register: netproj_a
      ignore_errors: true
      tags:
        - networking

    - name: Create netproj-a
      command: oc adm new-project netproj-a
      when: netproj_a | failed
      tags:
        - networking

    - name: Check for netproj-b
      command: oc get project netproj-b
      register: netproj_b
      ignore_errors: true
      tags:
        - networking

    - name: Create netproj-b
      command: oc adm new-project netproj-b
      when: netproj_b | failed
      tags:
        - networking

    - name: Ensure the projects are isolated for idempotency
      command: oc adm pod-network isolate-projects netproj-a
      tags:
        - networking
        - networking_test

    - name: Get the network ID of netproj-a
      command: oc get netnamespace netproj-a -o jsonpath='{.netid}'
      register: netproj_a
      tags:
        - networking
        - networking_test

    - name: Check that the network ID for netproj-b is different
      command: oc get netnamespace netproj-b -o jsonpath='{.netid}'
      register: netproj_b
      failed_when: netproj_a.stdout == netproj_b.stdout
      tags:
        - networking
        - networking_test

    - name: Check for ose dc in netproj-a
      command: oc get dc ose -n netproj-a
      ignore_errors: true
      tags:
        - networking
      register: ose_dc

    - name: Deploy ose pod into netproj-a
      command: oc create -f /opt/lab/support/ose.yaml -n netproj-a
      when: ose_dc | failed
      tags:
        - networking

    - name: Check for ose dc in netproj-b
      command: oc get dc ose -n netproj-b
      ignore_errors: true
      tags:
        - networking
      register: ose_dc

    - name: Deploy ose pod into netproj-b
      command: oc create -f /opt/lab/support/ose.yaml -n netproj-b
      when: ose_dc | failed
      tags:
        - networking

    - name: Wait for ose pods to be running
      command: oc get dc ose -n {{ item }} -o jsonpath='{.status.readyReplicas}'
      register: result
      until: '"1" in result.stdout'
      retries: 2
      delay: 60
      with_items:
        - "netproj-a"
        - "netproj-b"
      tags:
        - networking

    - name: Get the ip of the netproj-a pod
      shell: oc get pod -n netproj-a -o jsonpath='{.status.podIP}' $(oc get pod -n netproj-a | awk '/ose-*/{ print $1 }')
      register: pod_a
      tags:
        - networking
        - networking_test

    - name: Get the ip of the netproj-b pod
      shell: oc get pod -n netproj-b -o jsonpath='{.status.podIP}' $(oc get pod -n netproj-b | awk '/ose-*/{ print $1 }')
      register: pod_b
      tags:
        - networking
        - networking_test

    - name: Test connectivity from netproj-a to netproj-b
      shell: oc exec -n netproj-a $(oc get pod -n netproj-a | awk '/ose-*/{ print $1 }') -- ping -c1 -W1 {{ pod_b.stdout }}
      register: ping_b
      failed_when: ping_b | success
      tags:
        - networking
        - networking_test

    - name: Test connectivity from netproj-b to netproj-a
      shell: oc exec -n netproj-b $(oc get pod -n netproj-b | awk '/ose-*/{ print $1 }') -- ping -c1 -W1 {{ pod_a.stdout }}
      register: ping_a
      failed_when: ping_b | success
      tags:
        - networking
        - networking_test

    - name: Join the netproj-a and netproj-b networks
      command: oc adm pod-network join-projects netproj-a --to=netproj-b
      tags:
        - networking
        - networking_test

    - name: Get the network ID of netproj-a
      command: oc get netnamespace netproj-a -o jsonpath='{.netid}'
      register: netproj_a
      tags:
        - networking
        - networking_test

    - name: Check that the network ID for netproj-b is the same
      command: oc get netnamespace netproj-b -o jsonpath='{.netid}'
      register: netproj_b
      failed_when: netproj_a.stdout != netproj_b.stdout
      tags:
        - networking
        - networking_test

    - name: Get the ip of the netproj-a pod
      shell: oc get pod -n netproj-a -o jsonpath='{.status.podIP}' $(oc get pod -n netproj-a | awk '/ose-*/{ print $1 }')
      register: pod_a
      tags:
        - networking
        - networking_test

    - name: Get the ip of the netproj-b pod
      shell: oc get pod -n netproj-b -o jsonpath='{.status.podIP}' $(oc get pod -n netproj-b | awk '/ose-*/{ print $1 }')
      register: pod_b
      tags:
        - networking
        - networking_test

    - name: Test connectivity from netproj-a to netproj-b
      shell: oc exec -n netproj-a $(oc get pod -n netproj-a | awk '/ose-*/{ print $1 }') -- ping -c1 -W1 {{ pod_b.stdout }}
      register: ping_b
      tags:
        - networking
        - networking_test

    - name: Test connectivity from netproj-b to netproj-a
      shell: oc exec -n netproj-b $(oc get pod -n netproj-b | awk '/ose-*/{ print $1 }') -- ping -c1 -W1 {{ pod_a.stdout }}
      register: ping_a
      tags:
        - networking
        - networking_test

    - name: Make node03 unschedulable
      command: oc adm manage-node node03.internal.aws.testdrive.openshift.com --schedulable=false
      tags:
        - maintenance

    - name: Evacuate node03
      command: oc adm manage-node node03.internal.aws.testdrive.openshift.com --evacuate
      tags:
        - maintenance

    # instead of waiting like this, should probably find the gluster pod and then wait for it to be ready
    - name: Wait 120 seconds for evacuation and restart
      pause:
        seconds: 120
      tags:
        - maintenance

    - name: Make node03 schedulable again
      command: oc adm manage-node node03.internal.aws.testdrive.openshift.com --schedulable=true
      tags:
        - maintenance

- name: Put Registry on Persistent Storage
  hosts: localhost
  tags:
    - registry_storage_example
    - registry_storage
  tasks:
    - include_vars: /opt/lab/environment.yml

    - set_fact:
        cns_infra_storageclass: "{{CNS_INFRA_STORAGECLASS}}"

    - name: log back in as cluster admin into default namespace
      shell: oc login -u system:admin -n default

    - name: update registry deployment
      shell: oc volume dc/docker-registry --add --name=registry-storage -t pvc --claim-mode=ReadWriteMany --claim-size=10Gi --claim-class={{ cns_infra_storageclass }} --claim-name=registry-storage --overwrite

    - name: wait for pvc to be bound
      shell: oc get pvc/registry-storage -o jsonpath='{$.status.phase}'
      register: registry_pvc
      until: registry_pvc.stdout == "Bound"
      retries: 6
      delay: 5

    - name: wait for registry deployment to be ready
      shell: oc get rc -o jsonpath='{$.items[?(@.spec.selector.deploymentconfig=="docker-registry")].status.readyReplicas}'
      register: registry_rc_check
      until: registry_rc_check.stdout == "1"
      retries: 30
      delay: 10

      # without retry we suffer from the following error: Scaling the resource failed with: Operation cannot be fulfilled on deploymentconfigs "docker-registry": the object has been modified; please apply your changes to the latest version and try again; Current resource version Unknown
    - name: scale the registry deployment to 3
      shell: oc scale dc/docker-registry --replicas=3
      register: scale_dc
      until: scale_dc|success
      retries: 5
      delay: 10

    - name: wait for registry to be scaled to 3 pods
      shell: oc get rc docker-registry-$(oc get dc docker-registry -o jsonpath='{.status.latestVersion}') -o jsonpath='{.status.availableReplicas}'
      register: registry_scale_check
      until: registry_scale_check.stdout == "3"
      retries: 30
      delay: 10

- name: Verify Registry Persistent Storage
  hosts: localhost
  tags:
    - registry_storage
    - registry_storage_verify
  tasks:
    - include_vars: /opt/lab/environment.yml

    - name: check registry pvc from CNS
      shell: oc get pvc/registry-storage -o json
      register: get_registry_pvc
      changed_when: false

    - set_fact:
        registry_pvc: "{{ get_registry_pvc.stdout|from_json }}"

    - name: ensure registry pv comes from registry pvc
      shell: oc get dc/docker-registry -o jsonpath='{$.spec.template.spec.volumes[0].persistentVolumeClaim.claimName}'
      register: get_registry_pvc_name
      failed_when: get_registry_pvc_name.stdout != registry_pvc.metadata.name
